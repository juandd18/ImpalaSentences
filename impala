
//Queries without FROM, for testing purpose only

//sentencias con regex
SELECT 'abc123xyz' REGEXP '[[:digit:]]{3}';

SELECT regexp_replace('123456','(2|4|6)','x');

//time handler
SELECT now() + INTERVAL 3 DAYS + INTERVAL 5 HOURS;

//create table
create table sample_data
(id bigint,val int, webpage string, lastname string,val1 float, val2 float, 
lastname2 string, val12 string, val21 float, number bigint , number2 bigint)
row format delimited fields terminated by ",";

//shows statistics of the table
show table stats sample_data;

//shows structure of the table
describe formatted sample_data;

//insert data to the table location (where the hive table is reading data)
 hdfs dfs -put majestic_million.csv /user/hive/warehouse/juandavid.db/sample_data

//ALWAYS DO TO REFRESH
refresh sample_data;

// select to the table created
select max(val1) from sample_data where webpage = 'facebook.com' group by webpage;

//code to create view JUST read data does not create data in hdfs
create view normalized_view as
select one.id, one.val, one.zerofill, one.name,
one.assertion, two.id as location_id
from sample_data one join usa_cities two 1
on (one.city = two.city and one.state = two.state)

//MATERIALIZE view in a table
create table normalized_text
row format delimited fields terminated by ","
as select * from normalized_view;

//CONVERT TABLE TO A PARQUET FORMAT
create table normalized_parquet stored as parquet
as select * from sample_data;

//first we should compute the stats
compute stats normalized_parquet;
//stats of the columns
show column stats normalized_parquet;

//shows how impala makes the join or the select
explain select max(sample_copy.val1) from sample_copy join normalized_parquet using(id);

// "like" create a table with the same structure  
create table stats_demo_parquet
like partitioned_normalized_parquet; 1

//We use the [SHUFFLE] hint technique to avoid having each of the four nodes try to allocate (size of the file) GB-sized buffers to write 
//separate data files for all the partition values. The “shuffle” operation takes a little longer, 
//but avoids potential out-of-memory conditions.
insert into stats_demo_parquet partition (initial)
[shuffle] select * from partitioned_normalized_parquet 2
limit 1000000;

/////////////RECOMENDATIONS FOR SIZE REDUCTION///////////////
1.Converted the data to Parquet file format.
2.Normalized the data to reduce redundancy.
3.Partitioned the data to quickly locate ranges of values.
4.Computed the stats for both tables involved in the join query.
5.Referenced the partition key columns wherever practical in the query itself, especially in the join and WHERE clauses.
6.Used EXPLAIN to get an idea of the efficiency of possible queries as we iterated through several alternatives.



